{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 医薬情報テキストマイニングチャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [コンテスト詳細 ビッグデータ活用ならオプトDSL DeepAnalytics](https://deepanalytics.jp/compe/38/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('data/train.tsv')\n",
    "test = pd.read_table('data/test.tsv', header=None)\n",
    "num = len(data)\n",
    "test_num = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>便塊除去</td>\n",
       "      <td>摘便</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>アルツハイマ-認知症</td>\n",
       "      <td>アルツハイマー型認知症</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>熱傷（背部）</td>\n",
       "      <td>背部熱傷、程度不明（部位を限定しない）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015.8.12骨盤-その他-前立腺癌</td>\n",
       "      <td>前立腺癌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>口のにがみ</td>\n",
       "      <td>苦味</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>軽度呼吸困難感</td>\n",
       "      <td>呼吸困難</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>両足関節偽痛風</td>\n",
       "      <td>偽痛風</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>内痔核出血</td>\n",
       "      <td>痔出血</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>背部皮脂欠乏性湿疹</td>\n",
       "      <td>皮脂欠乏性湿疹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>左側頭葉出血</td>\n",
       "      <td>脳出血</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 before                after\n",
       "0                  便塊除去                   摘便\n",
       "1            アルツハイマ-認知症          アルツハイマー型認知症\n",
       "2                熱傷（背部）  背部熱傷、程度不明（部位を限定しない）\n",
       "3  2015.8.12骨盤-その他-前立腺癌                 前立腺癌\n",
       "4                 口のにがみ                   苦味\n",
       "5               軽度呼吸困難感                 呼吸困難\n",
       "6               両足関節偽痛風                  偽痛風\n",
       "7                 内痔核出血                  痔出血\n",
       "8             背部皮脂欠乏性湿疹              皮脂欠乏性湿疹\n",
       "9                左側頭葉出血                  脳出血"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4217"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['after'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecab をインストールする\n",
    "\n",
    "まずは、MeCabをインストール。\n",
    "\n",
    "       sudo apt-get install mecab mecab-naist-jdic mecab-ipadic-utf8 libmecab-dev\n",
    "       pip install mecab-python3\n",
    "\n",
    "Mecabには、医療辞書を使う(ComJisyo) \n",
    "[リリース ComeJisyo Linux用システム辞書 \\- ComeJisyo \\- OSDN](https://ja.osdn.net/projects/comedic/releases/44305)\n",
    "\n",
    "インストールでは、utf-8でインストールすることが必要。\n",
    "\n",
    "    ./configure --with-charset-=utf8　\n",
    "\n",
    "* [MeCabをUTF-8でインストールしたい。 - Qiita](http://qiita.com/junpooooow/items/0a7d13addc0acad10606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "#m = MeCab.Tagger(\"-Owakati\")\n",
    "m = MeCab.Tagger(\"-Ochasen\")\n",
    "#data['before'] = data['before'].apply(m.parseToNode)\n",
    "#test1 = test[0].apply(m.parseToNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "befores = set()\n",
    "afters = set()\n",
    "for i, row in data.iterrows():\n",
    "    node = m.parseToNode(row[0])\n",
    "    while node:\n",
    "        if node.feature.startswith('名詞'):\n",
    "            befores.add(node.surface)\n",
    "        node = node.next\n",
    "    afters.add(row[1])\n",
    "for i in test[0].iteritems():\n",
    "    node = m.parseToNode(i[1])\n",
    "    while node:\n",
    "        if node.feature.startswith('名詞'):\n",
    "            befores.add(node.surface)\n",
    "        node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752\n",
      "8752\n"
     ]
    }
   ],
   "source": [
    "print(len(befores))\n",
    "#befores.discard('(')\n",
    "#befores.discard(')')\n",
    "#befores.discard('（')\n",
    "#befores.discard('）')\n",
    "#befores.discard('[')\n",
    "#befores.discard(']')\n",
    "#befores.discard('」')\n",
    "#befores.discard('「')\n",
    "#befores.discard('【')\n",
    "#befores.discard('】')\n",
    "#befores.discard('『')\n",
    "#befores.discard('』')\n",
    "\n",
    "#befores.discard(\"0\")\n",
    "#befores.discard(\"1\")\n",
    "#befores.discard('2')\n",
    "#befores.discard('3')\n",
    "#befores.discard('4')\n",
    "#befores.discard('5')\n",
    "#befores.discard('6')\n",
    "#befores.discard('7')\n",
    "#befores.discard('8')\n",
    "#befores.discard('9')\n",
    "#befores.discard('2015')\n",
    "#befores.discard('12')\n",
    "\n",
    "#befores.discard('↓')\n",
    "#befores.discard('↑')\n",
    "#befores.discard('→')\n",
    "#befores.discard('←')\n",
    "#befores.discard('〜')\n",
    "#befores.discard('~')\n",
    "#befores.discard('.')\n",
    "#befores.discard(',')\n",
    "#befores.discard('（＇')\n",
    "#befores.discard('＞')\n",
    "#befores.discard('＜')\n",
    "#befores.discard('>')\n",
    "#befores.discard('<')\n",
    "#befores.discard('）(')\n",
    "#befores.discard('（+)')\n",
    "#befores.discard('（±）')\n",
    "#befores.discard('-')\n",
    "print(len(befores))\n",
    "vocab_size = len(befores)\n",
    "out_size = len(afters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "before_to_int, int_to_before = create_lookup_tables(befores)\n",
    "after_to_int, int_to_after = create_lookup_tables(afters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 1, 8.30282208588957, 7.0, 20.920562224970194)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = data['before'].apply(len)\n",
    "(lens.max(), lens.min(), lens.mean(), lens.median(), lens.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/carnd/anaconda3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  4.00000000e+00,   1.04000000e+02,   3.53000000e+02,\n",
       "          1.06500000e+03,   1.59700000e+03,   0.00000000e+00,\n",
       "          1.88700000e+03,   1.55900000e+03,   1.32000000e+03,\n",
       "          9.82000000e+02,   0.00000000e+00,   7.35000000e+02,\n",
       "          6.18000000e+02,   4.51000000e+02,   3.20000000e+02,\n",
       "          0.00000000e+00,   2.51000000e+02,   1.90000000e+02,\n",
       "          1.75000000e+02,   1.55000000e+02,   8.40000000e+01,\n",
       "          0.00000000e+00,   6.30000000e+01,   5.10000000e+01,\n",
       "          4.10000000e+01,   3.50000000e+01,   0.00000000e+00,\n",
       "          2.70000000e+01,   2.30000000e+01,   2.10000000e+01,\n",
       "          2.10000000e+01,   0.00000000e+00,   9.00000000e+00,\n",
       "          1.10000000e+01,   9.00000000e+00,   7.00000000e+00,\n",
       "          0.00000000e+00,   1.20000000e+01,   7.00000000e+00,\n",
       "          2.00000000e+00,   6.00000000e+00,   4.00000000e+00,\n",
       "          0.00000000e+00,   3.00000000e+00,   2.00000000e+00,\n",
       "          1.00000000e+00,   2.00000000e+00,   0.00000000e+00,\n",
       "          3.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          2.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00]),\n",
       " array([  1.  ,   1.81,   2.62,   3.43,   4.24,   5.05,   5.86,   6.67,\n",
       "          7.48,   8.29,   9.1 ,   9.91,  10.72,  11.53,  12.34,  13.15,\n",
       "         13.96,  14.77,  15.58,  16.39,  17.2 ,  18.01,  18.82,  19.63,\n",
       "         20.44,  21.25,  22.06,  22.87,  23.68,  24.49,  25.3 ,  26.11,\n",
       "         26.92,  27.73,  28.54,  29.35,  30.16,  30.97,  31.78,  32.59,\n",
       "         33.4 ,  34.21,  35.02,  35.83,  36.64,  37.45,  38.26,  39.07,\n",
       "         39.88,  40.69,  41.5 ,  42.31,  43.12,  43.93,  44.74,  45.55,\n",
       "         46.36,  47.17,  47.98,  48.79,  49.6 ,  50.41,  51.22,  52.03,\n",
       "         52.84,  53.65,  54.46,  55.27,  56.08,  56.89,  57.7 ,  58.51,\n",
       "         59.32,  60.13,  60.94,  61.75,  62.56,  63.37,  64.18,  64.99,\n",
       "         65.8 ,  66.61,  67.42,  68.23,  69.04,  69.85,  70.66,  71.47,\n",
       "         72.28,  73.09,  73.9 ,  74.71,  75.52,  76.33,  77.14,  77.95,\n",
       "         78.76,  79.57,  80.38,  81.19,  82.  ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbf7ef64fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "#trn = sequence.pad_sequences(trn, maxlen=13, value=0)\n",
    "#test = sequence.pad_sequences(test, maxlen=13, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "data['before'] = data['before'].apply(m.parse)\n",
    "trainX, trainY= [], []\n",
    "for i, row in data.iterrows():\n",
    "    xl = np.zeros(seq_len)\n",
    "    for j, w in enumerate(row['before'].split()):\n",
    "        if j < seq_len:\n",
    "            try:\n",
    "                xl[j] = before_to_int[w]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    trainX.append(xl)\n",
    "    trainY.append(after_to_int[row['after']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 2175.,  2199.,  2175.,     0.,  5747.,     0.,  5747.,     0.,\n",
       "          7701.,     0.,  7701.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([ 3726.,  3726.,  3726.,     0.,  2354.,  2354.,  2354.,     0.,\n",
       "          4526.,     0.,  4526.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([ 4456.,     0.,  4456.,     0.,     0.,     0.,     0.,     0.,\n",
       "          5976.,     0.,  5976.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([  144.,   144.,   144.,     0.,   976.,   976.,   976.,     0.,\n",
       "          1987.,  1987.,  1987.,     0.,   976.,   976.,   976.,     0.,\n",
       "           288.,   288.,   288.,     0.]),\n",
       "  array([ 1138.,     0.,  1138.,     0.,   916.,     0.,   916.,     0.,\n",
       "          2256.,     0.,  2256.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([ 3436.,     0.,  3436.,     0.,  4009.,     0.,  4009.,     0.,\n",
       "          8216.,     0.,  8216.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([ 5485.,     0.,  5485.,     0.,  1193.,     0.,  1193.,     0.,\n",
       "          5562.,     0.,  5562.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([  768.,     0.,   768.,     0.,  2992.,     0.,  2992.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.]),\n",
       "  array([ 5976.,     0.,  5976.,     0.,  3839.,     0.,  3839.,     0.,\n",
       "          1757.,     0.,  1757.,     0.,  3906.,     0.,  3906.,     0.,\n",
       "          8532.,     0.,  8532.,     0.]),\n",
       "  array([ 4548.,     0.,  4548.,     0.,  4278.,     0.,  4278.,     0.,\n",
       "          2992.,     0.,  2992.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.])],\n",
       " [2176, 2814, 3342, 3513, 449, 3207, 1950, 3471, 2708, 2849])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(trainX), len(trainY)\n",
    "trainX[:10], trainY[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testX = []\n",
    "test1 = test[0].apply(m.parse)\n",
    "\n",
    "for i, row in test1.iteritems():\n",
    "    xl = np.zeros(seq_len)\n",
    "    for j, w in enumerate(row.split()):\n",
    "        if j < seq_len:\n",
    "            try:\n",
    "                xl[j] = before_to_int[w]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    testX.append(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 20), (10000, 4217), (2225, 20), (2225, 4217), (12226, 20))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)\n",
    "testX = np.array(testX)\n",
    "validX = trainX[:2225]\n",
    "trainX = trainX[2225:]\n",
    "trainY = to_categorical(trainY)\n",
    "validY = trainY[:2225]\n",
    "trainY = trainY[2225:]\n",
    "trainX.shape, trainY.shape, validX.shape, validY.shape, testX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam, Adagrad, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(200, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(200, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(out_size, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 32)            280064    \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               128200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4217)              847617    \n",
      "=================================================================\n",
      "Total params: 1,297,681.0\n",
      "Trainable params: 1,296,881.0\n",
      "Non-trainable params: 800.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2225 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 6s - loss: 8.1288 - acc: 0.0199 - val_loss: 7.9953 - val_acc: 0.0310\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 6s - loss: 7.1913 - acc: 0.0607 - val_loss: 7.8087 - val_acc: 0.0449\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 6s - loss: 6.2441 - acc: 0.1164 - val_loss: 7.0389 - val_acc: 0.1312\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 6s - loss: 5.2915 - acc: 0.1894 - val_loss: 6.5991 - val_acc: 0.1897\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 6s - loss: 4.3669 - acc: 0.2638 - val_loss: 6.2373 - val_acc: 0.2274\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 6s - loss: 3.6205 - acc: 0.3410 - val_loss: 6.1973 - val_acc: 0.2611\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 6s - loss: 3.0063 - acc: 0.4072 - val_loss: 6.3931 - val_acc: 0.2773\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 6s - loss: 2.4783 - acc: 0.4811 - val_loss: 6.4145 - val_acc: 0.2840\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 6s - loss: 2.1206 - acc: 0.5274 - val_loss: 6.5225 - val_acc: 0.3047\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 6s - loss: 1.8217 - acc: 0.5817 - val_loss: 6.7657 - val_acc: 0.2957\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 6s - loss: 1.5722 - acc: 0.6261 - val_loss: 6.8678 - val_acc: 0.3074\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 6s - loss: 1.3556 - acc: 0.6699 - val_loss: 6.9256 - val_acc: 0.3137\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 6s - loss: 1.2453 - acc: 0.6933 - val_loss: 7.0368 - val_acc: 0.3182\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 6s - loss: 1.1315 - acc: 0.7158 - val_loss: 7.1286 - val_acc: 0.3200\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 6s - loss: 1.0258 - acc: 0.7427 - val_loss: 7.2438 - val_acc: 0.3213\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.9663 - acc: 0.7566 - val_loss: 7.2873 - val_acc: 0.3218\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.9119 - acc: 0.7686 - val_loss: 7.4268 - val_acc: 0.3213\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.8491 - acc: 0.7819 - val_loss: 7.3192 - val_acc: 0.3290\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.8352 - acc: 0.7847 - val_loss: 7.3787 - val_acc: 0.3303\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.7754 - acc: 0.7989 - val_loss: 7.4177 - val_acc: 0.3294\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.7510 - acc: 0.8058 - val_loss: 7.5633 - val_acc: 0.3353\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.7197 - acc: 0.8127 - val_loss: 7.4980 - val_acc: 0.3290\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.7054 - acc: 0.8146 - val_loss: 7.5819 - val_acc: 0.3245\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.6641 - acc: 0.8255 - val_loss: 7.5954 - val_acc: 0.3308\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.6760 - acc: 0.8285 - val_loss: 7.6107 - val_acc: 0.3380\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.6190 - acc: 0.8308 - val_loss: 7.5980 - val_acc: 0.3353\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.6314 - acc: 0.8328 - val_loss: 7.6198 - val_acc: 0.3290\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5830 - acc: 0.8434 - val_loss: 7.6404 - val_acc: 0.3308\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5704 - acc: 0.8460 - val_loss: 7.7052 - val_acc: 0.3330\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5760 - acc: 0.8436 - val_loss: 7.7050 - val_acc: 0.3321\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5371 - acc: 0.8599 - val_loss: 7.7060 - val_acc: 0.3362\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5374 - acc: 0.8589 - val_loss: 7.7143 - val_acc: 0.3375\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5244 - acc: 0.8591 - val_loss: 7.6779 - val_acc: 0.3438\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5273 - acc: 0.8617 - val_loss: 7.7473 - val_acc: 0.3393\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5337 - acc: 0.8604 - val_loss: 7.7640 - val_acc: 0.3344\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5282 - acc: 0.8594 - val_loss: 7.7077 - val_acc: 0.3371\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.5172 - acc: 0.8619 - val_loss: 7.8689 - val_acc: 0.3393\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4919 - acc: 0.8691 - val_loss: 7.6931 - val_acc: 0.3389\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4735 - acc: 0.8732 - val_loss: 7.7909 - val_acc: 0.3353\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4853 - acc: 0.8745 - val_loss: 7.6507 - val_acc: 0.3434\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4823 - acc: 0.8739 - val_loss: 7.7371 - val_acc: 0.3456\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4604 - acc: 0.8769 - val_loss: 7.7656 - val_acc: 0.3380\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4469 - acc: 0.8832 - val_loss: 7.7552 - val_acc: 0.3411\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4315 - acc: 0.8862 - val_loss: 7.7774 - val_acc: 0.3407\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4322 - acc: 0.8847 - val_loss: 7.7555 - val_acc: 0.3447\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4363 - acc: 0.8835 - val_loss: 7.7424 - val_acc: 0.3384\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4162 - acc: 0.8893 - val_loss: 7.8019 - val_acc: 0.3425\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4301 - acc: 0.8888 - val_loss: 7.7597 - val_acc: 0.3488\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.4007 - acc: 0.8918 - val_loss: 7.8304 - val_acc: 0.3452\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 6s - loss: 0.3818 - acc: 0.8954 - val_loss: 7.8701 - val_acc: 0.3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf569f26a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(validX, validY), epochs=50, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(out_size, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.compile(loss='categorical_crossentropy', optimizer=Nadam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.fit(trainX, trainY, validation_data=(validX, validY), epochs=5, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.save_weights('conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.load_weights('conv1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-size CNN\n",
    "* [Quid](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Merge\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = Concatenate()(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2 = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len),\n",
    "    BatchNormalization(),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Dropout(0.2),\n",
    "    graph,\n",
    "    Dropout(0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout (0.7),\n",
    "    Dense(out_size, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 50)            437600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 20, 50)            200       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              multiple                  39360     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               192100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4217)              425917    \n",
      "=================================================================\n",
      "Total params: 1,095,577.0\n",
      "Trainable params: 1,094,893.0\n",
      "Non-trainable params: 684.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.compile(loss='categorical_crossentropy', optimizer=Adam(0.005), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2225 samples\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 7s - loss: 2.0376 - acc: 0.5092 - val_loss: 6.6518 - val_acc: 0.3551\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.9906 - acc: 0.5165 - val_loss: 6.6628 - val_acc: 0.3515\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.9961 - acc: 0.5190 - val_loss: 6.6957 - val_acc: 0.3533\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.9868 - acc: 0.5205 - val_loss: 6.7187 - val_acc: 0.3546\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.9595 - acc: 0.5191 - val_loss: 6.7482 - val_acc: 0.3479\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.9538 - acc: 0.5261 - val_loss: 6.7159 - val_acc: 0.3497\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.8866 - acc: 0.5338 - val_loss: 6.7721 - val_acc: 0.3470\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.9033 - acc: 0.5355 - val_loss: 6.7353 - val_acc: 0.3551\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.8995 - acc: 0.5370 - val_loss: 6.7022 - val_acc: 0.3573\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.8393 - acc: 0.5448 - val_loss: 6.7254 - val_acc: 0.3551\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.8197 - acc: 0.5565 - val_loss: 6.7419 - val_acc: 0.3533\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.8209 - acc: 0.5522 - val_loss: 6.7345 - val_acc: 0.3510\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7961 - acc: 0.5579 - val_loss: 6.6878 - val_acc: 0.3546\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7819 - acc: 0.5563 - val_loss: 6.7248 - val_acc: 0.3555\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.8080 - acc: 0.5520 - val_loss: 6.7090 - val_acc: 0.3515\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7777 - acc: 0.5584 - val_loss: 6.7529 - val_acc: 0.3560\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7847 - acc: 0.5621 - val_loss: 6.7557 - val_acc: 0.3578\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7519 - acc: 0.5648 - val_loss: 6.7618 - val_acc: 0.3510\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7128 - acc: 0.5744 - val_loss: 6.7664 - val_acc: 0.3600\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 7s - loss: 1.7133 - acc: 0.5737 - val_loss: 6.7641 - val_acc: 0.3618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf31e6a630>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.fit(trainX, trainY, validation_data=(validX, validY), epochs=20, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.save_weights('conv2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.load_weights('conv2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.fit(trainX, trainY, validation_data=(validX, validY), epochs=5, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = conv2.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12226, 4217)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('白内障手術', 2741)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = np.argmax(preds[0])\n",
    "int_to_after[i1], i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(columns=[\"before\", \"after\"])\n",
    "for i in range(0, test_num):\n",
    "    for id in np.argsort(preds[i])[::-1][:3]:\n",
    "        series = pd.DataFrame([[test[0][i], int_to_after[id]]], columns=out.columns)\n",
    "        out = out.append(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.to_csv(\"submission_pseude.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_label = np.concatenate([trainY, preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_feat = np.concatenate([trainX, testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22226, 20), (22226, 4217))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_feat.shape, comb_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22226 samples, validate on 2225 samples\n",
      "Epoch 1/50\n",
      "22226/22226 [==============================] - 13s - loss: 7.1766 - acc: 0.0882 - val_loss: 7.6194 - val_acc: 0.0773\n",
      "Epoch 2/50\n",
      "22226/22226 [==============================] - 12s - loss: 5.5552 - acc: 0.2163 - val_loss: 6.4190 - val_acc: 0.2076\n",
      "Epoch 3/50\n",
      "22226/22226 [==============================] - 12s - loss: 4.5542 - acc: 0.3269 - val_loss: 6.0862 - val_acc: 0.2661\n",
      "Epoch 4/50\n",
      "22226/22226 [==============================] - 12s - loss: 3.9519 - acc: 0.4020 - val_loss: 5.9802 - val_acc: 0.3025\n",
      "Epoch 5/50\n",
      "22226/22226 [==============================] - 12s - loss: 3.5541 - acc: 0.4562 - val_loss: 5.9628 - val_acc: 0.3133\n",
      "Epoch 6/50\n",
      "22226/22226 [==============================] - 12s - loss: 3.2895 - acc: 0.5019 - val_loss: 5.9699 - val_acc: 0.3330\n",
      "Epoch 7/50\n",
      "22226/22226 [==============================] - 12s - loss: 3.0585 - acc: 0.5353 - val_loss: 6.0837 - val_acc: 0.3371\n",
      "Epoch 8/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.8985 - acc: 0.5660 - val_loss: 6.1234 - val_acc: 0.3393\n",
      "Epoch 9/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.7700 - acc: 0.5937 - val_loss: 6.1436 - val_acc: 0.3465\n",
      "Epoch 10/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.6681 - acc: 0.6154 - val_loss: 6.2071 - val_acc: 0.3537\n",
      "Epoch 11/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.5711 - acc: 0.6376 - val_loss: 6.2216 - val_acc: 0.3528\n",
      "Epoch 12/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.5148 - acc: 0.6492 - val_loss: 6.2898 - val_acc: 0.3519\n",
      "Epoch 13/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.4544 - acc: 0.6624 - val_loss: 6.3342 - val_acc: 0.3564\n",
      "Epoch 14/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.3941 - acc: 0.6790 - val_loss: 6.3005 - val_acc: 0.3631\n",
      "Epoch 15/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.3397 - acc: 0.6889 - val_loss: 6.2506 - val_acc: 0.3587\n",
      "Epoch 16/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.3006 - acc: 0.6967 - val_loss: 6.2297 - val_acc: 0.3622\n",
      "Epoch 17/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.2821 - acc: 0.6992 - val_loss: 6.3222 - val_acc: 0.3609\n",
      "Epoch 18/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.2420 - acc: 0.7133 - val_loss: 6.2831 - val_acc: 0.3654\n",
      "Epoch 19/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.2148 - acc: 0.7125 - val_loss: 6.2665 - val_acc: 0.3658\n",
      "Epoch 20/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.1936 - acc: 0.7180 - val_loss: 6.3356 - val_acc: 0.3694\n",
      "Epoch 21/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.1709 - acc: 0.7266 - val_loss: 6.3292 - val_acc: 0.3600\n",
      "Epoch 22/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.1480 - acc: 0.7270 - val_loss: 6.3312 - val_acc: 0.3645\n",
      "Epoch 23/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.1238 - acc: 0.7344 - val_loss: 6.3863 - val_acc: 0.3640\n",
      "Epoch 24/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.1142 - acc: 0.7380 - val_loss: 6.3428 - val_acc: 0.3676\n",
      "Epoch 25/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.0911 - acc: 0.7420 - val_loss: 6.3647 - val_acc: 0.3613\n",
      "Epoch 26/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.0686 - acc: 0.7469 - val_loss: 6.3986 - val_acc: 0.3569\n",
      "Epoch 27/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.0705 - acc: 0.7418 - val_loss: 6.3785 - val_acc: 0.3649\n",
      "Epoch 28/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.0421 - acc: 0.7470 - val_loss: 6.3972 - val_acc: 0.3645\n",
      "Epoch 29/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.0288 - acc: 0.7534 - val_loss: 6.3838 - val_acc: 0.3721\n",
      "Epoch 30/50\n",
      "22226/22226 [==============================] - 12s - loss: 2.0119 - acc: 0.7552 - val_loss: 6.3866 - val_acc: 0.3649\n",
      "Epoch 31/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9959 - acc: 0.7594 - val_loss: 6.4106 - val_acc: 0.3631\n",
      "Epoch 32/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9988 - acc: 0.7558 - val_loss: 6.5159 - val_acc: 0.3622\n",
      "Epoch 33/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9911 - acc: 0.7617 - val_loss: 6.4579 - val_acc: 0.3735\n",
      "Epoch 34/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9651 - acc: 0.7683 - val_loss: 6.4832 - val_acc: 0.3663\n",
      "Epoch 35/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9667 - acc: 0.7692 - val_loss: 6.5243 - val_acc: 0.3676\n",
      "Epoch 36/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9541 - acc: 0.7677 - val_loss: 6.4843 - val_acc: 0.3681\n",
      "Epoch 37/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9420 - acc: 0.7689 - val_loss: 6.4915 - val_acc: 0.3699\n",
      "Epoch 38/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9256 - acc: 0.7735 - val_loss: 6.4818 - val_acc: 0.3712\n",
      "Epoch 39/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9304 - acc: 0.7742 - val_loss: 6.5010 - val_acc: 0.3645\n",
      "Epoch 40/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9175 - acc: 0.7751 - val_loss: 6.5240 - val_acc: 0.3699\n",
      "Epoch 41/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.9086 - acc: 0.7765 - val_loss: 6.5371 - val_acc: 0.3699\n",
      "Epoch 42/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8951 - acc: 0.7754 - val_loss: 6.5088 - val_acc: 0.3699\n",
      "Epoch 43/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8987 - acc: 0.7749 - val_loss: 6.4944 - val_acc: 0.3676\n",
      "Epoch 44/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8772 - acc: 0.7847 - val_loss: 6.5030 - val_acc: 0.3658\n",
      "Epoch 45/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8798 - acc: 0.7808 - val_loss: 6.5256 - val_acc: 0.3667\n",
      "Epoch 46/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8695 - acc: 0.7863 - val_loss: 6.5061 - val_acc: 0.3739\n",
      "Epoch 47/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8579 - acc: 0.7871 - val_loss: 6.5076 - val_acc: 0.3658\n",
      "Epoch 48/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8549 - acc: 0.7852 - val_loss: 6.5212 - val_acc: 0.3676\n",
      "Epoch 49/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8458 - acc: 0.7910 - val_loss: 6.5076 - val_acc: 0.3694\n",
      "Epoch 50/50\n",
      "22226/22226 [==============================] - 12s - loss: 1.8451 - acc: 0.7875 - val_loss: 6.5151 - val_acc: 0.3726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf31665f98>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(comb_feat, comb_label, validation_data=(validX, validY), epochs=50, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
